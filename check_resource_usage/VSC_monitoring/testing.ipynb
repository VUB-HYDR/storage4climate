{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.copy_on_write = True\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCLIMATE\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#       Settings        #\n",
    "#########################\n",
    "\n",
    "\n",
    "year = '2025'\n",
    "\n",
    "d_quarters = {'Q1': pd.date_range(start=pd.to_datetime(\"2025-02-08\"),end=pd.to_datetime(\"2025-05-08\")), \n",
    "              'Q2': pd.date_range(start=pd.to_datetime(\"2025-05-08\"),end=pd.to_datetime(\"2025-08-08\")), \n",
    "              'Q3': pd.date_range(start=pd.to_datetime(\"2025-08-08\"),end=pd.to_datetime(\"2025-11-08\")), \n",
    "              'Q4': pd.date_range(start=pd.to_datetime(\"2025-11-08\"),end=pd.to_datetime(\"2026-02-08\")), \n",
    "              'total': pd.date_range(start=pd.to_datetime(\"2025-02-08\"),end=pd.to_datetime(\"2026-02-08\")) }\n",
    "\n",
    "date = pd.Timestamp.today().normalize()\n",
    "\n",
    "for quarter, dates in d_quarters.items():\n",
    "    if quarter == \"total\":\n",
    "        continue\n",
    "    if date in dates:\n",
    "        current_quarter = quarter\n",
    "\n",
    "time_range_previous_cycle = pd.date_range(start=pd.to_datetime(\"2024-02-08\"),end=pd.to_datetime(\"2025-02-08\"))\n",
    "\n",
    "d_projects = {'BCLIMATE'  :  '2022_201', \n",
    "              'RCS'       :  '2022_202', \n",
    "              'RSDA'      :  '2022_203',\n",
    "              'H-CEL'     :  '2022_204',\n",
    "              'RMIB-UGENT':  '2022_205'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "#        Inputs         #\n",
    "#########################\n",
    "\n",
    "#Get the git directory\n",
    "path_monitoring = f\"\"\n",
    "# load requested resources in S4C proposal\n",
    "\n",
    "df_requested_2024 = pd.read_csv(f\"{path_monitoring}requested_resources_2024.csv\", delimiter=\";\",index_col=0)\n",
    "df_transferred_2024 = pd.read_csv(f\"{path_monitoring}transferred_resources_2024.csv\", delimiter=\";\",index_col=0)\n",
    "\n",
    "df_used_2024 = pd.read_csv(f\"{path_monitoring}used_resources_2024.csv\", delimiter=\";\",index_col=0)\n",
    "\n",
    "# the final amount of credits available is the sum of the requested credits in the proposal and the transferred credits from the previous year, minus the use of the previous year. \n",
    "\n",
    "ds_left_from_2024 = df_requested_2024['total']+ df_transferred_2024['total']  - df_used_2024['total']\n",
    "\n",
    "# redistribute what is left of 2024 evenly over quarters\n",
    "df_left_from_2024 = ds_left_from_2024.to_frame(name='total')\n",
    "\n",
    "for quarter in ['Q1', 'Q2', 'Q3', 'Q4']:\n",
    "    df_left_from_2024[quarter] = df_left_from_2024['total'].div(4)\n",
    "\n",
    "df_left_from_2024.reindex(columns=['Q1', 'Q2', 'Q3', 'Q4','total'])\n",
    "\n",
    "# load requested resources in S4C proposal in 2025\n",
    "df_requested_year = pd.read_csv(f\"{path_monitoring}requested_resources_{year}.csv\", delimiter=\";\",index_col=0)\n",
    "df_transferred_year = pd.read_csv(f\"{path_monitoring}transferred_resources_{year}.csv\", delimiter=\";\",index_col=0)\n",
    "\n",
    "\n",
    "df_requested = df_left_from_2024 + df_requested_year + df_transferred_year\n",
    "\n",
    "\n",
    "# select one project group\n",
    "\n",
    "#for i, group in enumerate(d_projects.keys()): \n",
    "i=0\n",
    "group = list(d_projects.keys())[0]\n",
    "\n",
    "print(group)\n",
    "project = d_projects[group]\n",
    "\n",
    "quarter_credits = df_requested.loc[group]\n",
    "\n",
    "\n",
    "# read the input files\n",
    "path = rf'{path_monitoring}input'\n",
    "all_files = glob.glob(f'{path}/*{project}.csv')\n",
    "\n",
    "df = pd.read_csv(all_files[0], header=1)\n",
    "\n",
    "#########################\n",
    "#     Pre-processing    #\n",
    "#########################\n",
    "\n",
    "\n",
    "#Get the index of the row containing project logs in resource column\n",
    "index_logs = df[df['Resource'] == 'Project Logs'].index[0]\n",
    "df_overview = df[:index_logs]\n",
    "df_credits = df_overview[df_overview['Resource'] == 'credits']\n",
    "df_credits['Total_used'] = df_credits['Current used by user'].str.replace(' Hours', '').astype(float)\n",
    "df_credits['Cumulative_used_by_user'] = df_credits['Total used by user'].str.replace(' Hours', '').astype(float)\n",
    "df_credits['Cumulative_total_used'] = df_credits['Total used'].str.replace(' Hours', '').astype(float)\n",
    "df_credits['Date'] =  pd.to_datetime(pd.to_datetime(df_credits['Date'], format='%d/%m/%Y - %H:%M').dt.date)\n",
    "df_credits = df_credits.sort_values(by='Date')\n",
    "\n",
    "Total_credits = quarter_credits['total']\n",
    "\n",
    "#Make a dictionary with the remaining credits per quarter by subtracting the total used from the total credits per quarter until there are no credits left\n",
    "used_credits = quarter_credits.copy().loc[['Q1','Q2','Q3','Q4']]\n",
    "\n",
    "\n",
    "used_credits_previouscycle = (df_credits[df_credits['Date'].isin(time_range_previous_cycle)]['Total_used']).sum()\n",
    "\n",
    "\n",
    "for key in d_quarters.keys():\n",
    "\n",
    "    df_credits_quarter = df_credits[df_credits['Date'].isin(d_quarters[key])]\n",
    "    used_credits[key] = (df_credits_quarter['Total_used']).sum()\n",
    "    \n",
    "\n",
    "\n",
    "# save loaded credits for stacked bar plot\n",
    "if i==0: \n",
    "    df_requested_credits = quarter_credits.to_frame()\n",
    "    df_used_credits = used_credits.to_frame()\n",
    "else: \n",
    "\n",
    "    df_requested_credits = df_requested_credits.merge(quarter_credits.to_frame(), left_index=True, right_index=True)\n",
    "    df_used_credits = df_used_credits.merge(used_credits.to_frame(), left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = rf'{path_monitoring}input'\n",
    "all_files = glob.glob(f'{path}/*{project}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input/usage_20251217_2022_201.csv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Passed header=1 but only 1 lines in file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_monitoring\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m all_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(all_files[\u001b[38;5;241m0\u001b[39m], header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/dodrio/scratch/projects/2022_200/project_output/rcs/software/miniconda3/envs/env_geo/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/dodrio/scratch/projects/2022_200/project_output/rcs/software/miniconda3/envs/env_geo/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/dodrio/scratch/projects/2022_200/project_output/rcs/software/miniconda3/envs/env_geo/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/dodrio/scratch/projects/2022_200/project_output/rcs/software/miniconda3/envs/env_geo/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/dodrio/scratch/projects/2022_200/project_output/rcs/software/miniconda3/envs/env_geo/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:677\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Passed header=1 but only 1 lines in file"
     ]
    }
   ],
   "source": [
    "    # read the input files\n",
    "    path = rf'{path_monitoring}input'\n",
    "    all_files = glob.glob(f'{path}/*{project}.csv')\n",
    "\n",
    "    df = pd.read_csv(all_files[0], header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine credits used in previous cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCLIMATE\n",
      "RCS\n",
      "RSDA\n",
      "H-CEL\n",
      "RMIB-UGENT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# determine used credits in previous cycle (2024)\n",
    "\n",
    "for i, group in enumerate(d_projects.keys()): \n",
    "    print(group)\n",
    "    project = d_projects[group]\n",
    "\n",
    "    # read the input files\n",
    "    path = rf'{path_monitoring}input'\n",
    "    all_files = glob.glob(f'{path}/*{project}.csv')\n",
    "\n",
    "    df = pd.read_csv(all_files[0], header=1)\n",
    "\n",
    "    #Get the index of the row containing project logs in resource column\n",
    "    index_logs = df[df['Resource'] == 'Project Logs'].index[0]\n",
    "    df_overview = df[:index_logs]\n",
    "    df_credits = df_overview[df_overview['Resource'] == 'credits']\n",
    "    df_credits['Total_used'] = df_credits['Current used by user'].str.replace(' Hours', '').astype(float)\n",
    "    df_credits['Cumulative_used_by_user'] = df_credits['Total used by user'].str.replace(' Hours', '').astype(float)\n",
    "    df_credits['Cumulative_total_used'] = df_credits['Total used'].str.replace(' Hours', '').astype(float)\n",
    "    df_credits['Date'] =  pd.to_datetime(pd.to_datetime(df_credits['Date'], format='%d/%m/%Y - %H:%M').dt.date)\n",
    "    df_credits = df_credits.sort_values(by='Date')\n",
    "\n",
    "    used_credits_previouscycle = (df_credits[df_credits['Date'].isin(time_range_previous_cycle)]['Total_used']).sum()\n",
    "    df_used_2024.loc[group,\"total\"] = int(used_credits_previouscycle)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
